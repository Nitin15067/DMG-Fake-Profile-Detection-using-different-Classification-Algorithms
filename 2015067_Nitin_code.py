# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nQo_DEQwhSusTWi5R5qaj5Zas50uPXx4
"""

#import necessary modules
from google.colab import files
uploaded = files.upload()

import io
import pandas as pd
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from sklearn.metrics import matthews_corrcoef
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import cross_val_predict
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn import svm
import warnings

warnings.filterwarnings('ignore')


d = pd.read_csv(io.BytesIO(uploaded['train.csv']))
d2 = pd.read_csv(io.BytesIO(uploaded['test.csv']))

labels = d.columns.tolist()

df = d.sample(frac=1).reset_index(drop=True)
df_fake_exception = df.drop('fake',1)

# for individual columns classification
individual_pred_dict = {}

for l in labels:
  
  if l == 'fake':
    break
  try:
    df_l = pd.DataFrame(df[l])

    individual_pred_dict[l] = cross_val_predict(RandomForestClassifier(), df_l, df['fake'], cv=10)
  except ValueError as v:
    individual_pred_dict[l] = None

results_individual_dict = {}

for key, pred in individual_pred_dict.items():
  try:
			# Accuracy
    acc = accuracy_score(df['fake'], pred)
  except ValueError:
    acc = 0
  try:
    # Precision
    pres = precision_score(df['fake'], pred, average='binary')
  except ValueError:
    pres = 0
  try:
    # Recall
    rec = recall_score(df['fake'], pred, average='binary')
  except ValueError:
    rec = 0
  try:
    # F-measure
    f1 = f1_score(df['fake'], pred, average='binary')
  except ValueError:
    f1 = 0
  try:
    # MCC
    mcc = matthews_corrcoef(df['fake'], pred) 
  except ValueError:
    mcc = 0
  try:
    # AUC
    auc_score = roc_auc_score(df['fake'], pred)
  except ValueError:
    auc_score = 0
    # 
    # Coud be this one must be rechecked with paper
    # from sklearn.metrics import roc_auc_score
    # roc_auc_score(labels, y_scores)
  results_individual_dict[key] = [acc,pres,rec,f1,mcc,auc_score]

print("-----------------   Results from Individual attribute classification   -----------------")
for key,value in results_individual_dict.items():
  print("Attribute: "+key)
  print("Accuracy: "+ str(round(value[0],3)))
  print("Precision: "+ str(round(value[1],3)))
  print("Recall: "+ str(round(value[2],3)))
  print(" F-M: "+str(round(value[3],3)))
  print("MCC: "+ str(round(value[4],3)))
  print("AUC: "+str(round(value[5],3)))
  print()

# for training data classification
classifiers_dict = {}
predictions_dict = {}

classifiers_dict['RF'] = RandomForestClassifier()
classifiers_dict['J48'] = DecisionTreeClassifier()
classifiers_dict['AB'] = AdaBoostClassifier()
classifiers_dict['kNN'] = KNeighborsClassifier()
classifiers_dict['LR'] = LogisticRegression()
classifiers_dict['SVM'] = svm.SVC(kernel='rbf', C=10, gamma = 'auto')

for key, value in classifiers_dict.items():
  try:
    predictions_dict[key] = cross_val_predict(value, df_fake_exception, df['fake'], cv=10)
  except ValueError as v:
    predictions_dict[key] = None

results_dict = {}
for key, pred in predictions_dict.items():
  try:
	  acc = accuracy_score(df['fake'], pred)
  except ValueError:
    acc = 0
  try:
    pres = precision_score(df['fake'], pred, average='binary')
  except ValueError:
    pres = 0
  try:
    rec = recall_score(df['fake'], pred, average='binary')
  except ValueError:
    rec = 0
  try:
    f1 = f1_score(df['fake'], pred, average='binary')
  except ValueError:
    f1 = 0
  try:
    mcc = matthews_corrcoef(df['fake'], pred) 
  except ValueError:
    mcc = 0
  try:
    auc_score = roc_auc_score(df['fake'], pred)
  except ValueError:
    auc_score = 0
  
  results_dict[key] = [acc,pres,rec,f1,mcc,auc_score]

print("----------------   Results from training data ------------------- ")
for key,value in results_dict.items():
  print("Algorithm: "+key)
  print("Accuracy: "+ str(round(value[0],3)))
  print("Precision: "+str(round(value[1],3)))
  print("Recall: "+ str(round(value[2],3)))
  print(" F-M: "+str(round(value[3],3)))
  print("MCC: "+ str(round(value[4],3)))
  print("AUC: "+str(round(value[5],3)))
  print()

training_predictions_dict = {}
training_predictions_dict = predictions_dict

# for testing data classification

df = d2.sample(frac=1).reset_index(drop=True)
df_fake_exception = df.drop('fake',1)

classifiers_dict = {}
predictions_dict = {}

classifiers_dict['RF'] = RandomForestClassifier()
classifiers_dict['J48'] = DecisionTreeClassifier()
classifiers_dict['AB'] = AdaBoostClassifier()
classifiers_dict['kNN'] = KNeighborsClassifier()
classifiers_dict['LR'] = LogisticRegression()
classifiers_dict['SVM'] = svm.SVC(kernel='rbf', C=10, gamma = 'auto')

for key, value in classifiers_dict.items():
  try:
    predictions_dict[key] = cross_val_predict(value, df_fake_exception, df['fake'], cv=10)
  except ValueError as v:
    predictions_dict[key] = None

results_dict = {}
for key, pred in predictions_dict.items():
  try:
			# Accuracy
    acc = accuracy_score(df['fake'], pred)
  except ValueError:
    acc = 0
  try:
    # Precision
    pres = precision_score(df['fake'], pred, average='binary')
  except ValueError:
    pres = 0
  try:
    # Recall
    rec = recall_score(df['fake'], pred, average='binary')
  except ValueError:
    rec = 0
  try:
    # F-measure
    f1 = f1_score(df['fake'], pred, average='binary')
  except ValueError:
    f1 = 0
  try:
    mcc = matthews_corrcoef(df['fake'], pred) 
  except ValueError:
    mcc = 0
  try:
    auc_score = roc_auc_score(df['fake'], pred)
  except ValueError:
    auc_score = 0
  results_dict[key] = [acc,pres,rec,f1,mcc,auc_score]

print("--------------  Results from testing data -----------------")
for key,value in results_dict.items():
  print("Algorithm: "+key)
  print("Accuracy: "+ str(round(value[0],3)))
  print("Precision: "+str(round(value[1],3)))
  print("Recall: "+ str(round(value[2],3)))
  print(" F-M: "+str(round(value[3],3)))
  print("MCC: "+ str(round(value[4],3)))
  print("AUC: "+str(round(value[5],3)))
  print()

testing_predictions_dict = {}
testing_predictions_dict = predictions_dict

